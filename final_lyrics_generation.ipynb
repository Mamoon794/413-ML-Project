{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51c1e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import sentencepiece as spm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mainPath = \"/virtual/akhtar79/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9cd04c",
   "metadata": {},
   "source": [
    "## Preprocess Song Lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97833ef2",
   "metadata": {},
   "source": [
    "The code below provides a function to clean lyrics by removing annotations (e.g. [Chorus], [Verse 1], etc), parenthesis, quotation marks, extra whitespace and newlines, non-ASCII characters, and other special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e0eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bracketed_annotations(text):\n",
    "    text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
    "    text = re.sub(r\"\\(.*?\\)\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def remove_quotation_marks(text):\n",
    "    return re.sub(r'[\"\"\"\\']', '', text)\n",
    "\n",
    "def convert_newlines_to_v_tokens(text):\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n",
    "    text = re.sub(r\"\\n+\", \" <V> \", text)\n",
    "    return text\n",
    "\n",
    "def normalize_whitespace(text):\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    return text.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "\n",
    "def remove_standalone_punctuation(text):\n",
    "    text = re.sub(r'\\s+([.,!?;:])\\s+', r' \\1 ', text)\n",
    "    return re.sub(r'\\s+([\\'\"])\\s+', ' ', text)\n",
    "\n",
    "def remove_duplicate_punctuation(text):\n",
    "    text = re.sub(r'([.,!?;:]){2,}', r'\\1', text)\n",
    "    return text\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[(){}\\[\\]<>]', '', text)\n",
    "\n",
    "\n",
    "def clean_lyrics(text):\n",
    "    \"\"\"Clean and normalize song lyrics by removing annotations, special characters, and normalizing whitespace.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = remove_bracketed_annotations(text)\n",
    "    text = remove_quotation_marks(text)\n",
    "    text = convert_newlines_to_v_tokens(text)\n",
    "    text = normalize_whitespace(text)\n",
    "    text = remove_non_ascii(text)\n",
    "    text = text.lower()\n",
    "    text = remove_standalone_punctuation(text)\n",
    "    text = remove_duplicate_punctuation(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = normalize_whitespace(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11552ef3",
   "metadata": {},
   "source": [
    "The code below filters the songs dataset to retain only English songs and specific genres. It also cleans the lyrics using the function above and removes lyrics that are too long or too short. Then, it adds a genre token to each lyric and saves to a CSV 'english_cleaned_songs.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd36a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_language_and_genre(chunk):\n",
    "    allowed_genres = [\"rap\", \"pop\", \"rock\", \"rb\", \"country\"]\n",
    "    filtered = chunk[(chunk[\"language\"] == \"en\") & (chunk[\"tag\"] != \"misc\")].copy()\n",
    "    filtered = filtered[filtered[\"tag\"].isin(allowed_genres)]\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def apply_lyrics_cleaning(chunk):\n",
    "    chunk[\"clean_lyrics\"] = chunk[\"lyrics\"].apply(clean_lyrics)\n",
    "    return chunk\n",
    "\n",
    "\n",
    "def filter_lyrics_by_word_count(chunk, min_words, max_words):\n",
    "    chunk[\"word_count\"] = chunk[\"clean_lyrics\"].str.split().str.len()\n",
    "    chunk = chunk[\n",
    "        (chunk[\"word_count\"] >= min_words) &\n",
    "        (chunk[\"word_count\"] <= max_words)\n",
    "    ]\n",
    "    chunk = chunk.drop(columns=[\"word_count\"])\n",
    "    return chunk\n",
    "\n",
    "\n",
    "def add_genre_token_to_lyrics(chunk):\n",
    "    chunk[\"clean_lyrics\"] = \"<\" + chunk[\"tag\"].str.upper() + \"> \" + chunk[\"clean_lyrics\"]\n",
    "    return chunk\n",
    "\n",
    "\n",
    "def save_chunk_to_csv(chunk, output_csv, is_first_chunk):\n",
    "    if is_first_chunk:\n",
    "        chunk.to_csv(output_csv, index=False, mode='w')\n",
    "    else:\n",
    "        chunk.to_csv(output_csv, index=False, mode='a', header=False)\n",
    "\n",
    "\n",
    "def clean_and_filter_lyrics_dataset(\n",
    "    input_csv='/content/genius_dataset/song_lyrics.csv',\n",
    "    output_csv='english_cleaned_songs.csv',\n",
    "    min_words=50,\n",
    "    max_words=1000\n",
    "):\n",
    "    \"\"\"Process lyrics dataset in chunks: filter by language/genre, clean text, add genre tokens, and save to CSV.\"\"\"\n",
    "\n",
    "    chunksize = 100000\n",
    "    is_first_chunk = True\n",
    "    chunks_processed = 0\n",
    "    \n",
    "    print(\"Processing dataset in chunks...\")\n",
    "    \n",
    "    for chunk in pd.read_csv(input_csv, chunksize=chunksize):\n",
    "        chunk = filter_by_language_and_genre(chunk)\n",
    "        chunk = apply_lyrics_cleaning(chunk)\n",
    "        chunk = filter_lyrics_by_word_count(chunk, min_words, max_words)\n",
    "        chunk = add_genre_token_to_lyrics(chunk)\n",
    "        \n",
    "        save_chunk_to_csv(chunk, output_csv, is_first_chunk)\n",
    "        \n",
    "        is_first_chunk = False\n",
    "        chunks_processed += 1\n",
    "        print(f\"Processed {chunks_processed} chunk(s)\")\n",
    "    \n",
    "    print(\"Cleaning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fa4c83",
   "metadata": {},
   "source": [
    "Balance the dataset so that we have equal number of songs per genre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039c8380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_genres(input_csv=\"english_cleaned_songs.csv\", \n",
    "                   output_csv=\"english_cleaned_reduced.csv\", \n",
    "                   max_per_genre=85000):\n",
    "    \"\"\"\n",
    "    Balance dataset to have equal samples per genre\n",
    "    \"\"\"\n",
    "    allowed_genres = [\"rap\", \"pop\", \"rock\", \"rb\", \"country\"]\n",
    "    counts = {genre: 0 for genre in allowed_genres}\n",
    "    chunksize = 100000\n",
    "    first_write = True\n",
    "    \n",
    "    print(\"Balancing genres...\")\n",
    "    for chunk in pd.read_csv(input_csv, chunksize=chunksize):\n",
    "        chunk = chunk[chunk[\"tag\"].isin(allowed_genres)]\n",
    "        chunk = chunk[chunk[\"tag\"].apply(lambda x: counts[x] < max_per_genre)]\n",
    "        \n",
    "        if chunk.empty:\n",
    "            continue\n",
    "        \n",
    "        sampled_chunks = []\n",
    "        for genre, group in chunk.groupby(\"tag\"):\n",
    "            remaining = max_per_genre - counts[genre]\n",
    "            if len(group) > remaining:\n",
    "                group = group.sample(n=remaining, random_state=42)\n",
    "            counts[genre] += len(group)\n",
    "            sampled_chunks.append(group)\n",
    "        \n",
    "        final_chunk = pd.concat(sampled_chunks)\n",
    "        final_chunk.to_csv(output_csv, mode='w' if first_write else 'a', \n",
    "                          index=False, header=first_write)\n",
    "        first_write = False\n",
    "        \n",
    "        if all(count >= max_per_genre for count in counts.values()):\n",
    "            break\n",
    "    \n",
    "    print(f\"Balanced dataset saved: {output_csv}\")\n",
    "    print(\"Final counts per genre:\", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13be2511",
   "metadata": {},
   "source": [
    "Create training and validation datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34589fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_splits(csv_file=\"english_cleaned_reduced.csv\"):\n",
    "    \"\"\"\n",
    "    Split data into train and val sets\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    train, val = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    train[\"clean_lyrics\"].to_csv(mainPath+\"train.txt\", index=False, header=False)\n",
    "    val[\"clean_lyrics\"].to_csv(mainPath+\"val.txt\", index=False, header=False)\n",
    "    \n",
    "    print(f\"Train: {len(train)}, Val: {len(val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38698c7c",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3ed2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer(vocab_size=10000):\n",
    "    \"\"\"\n",
    "    Train SentencePiece tokenizer with specified vocabulary size\n",
    "    \"\"\"\n",
    "    print(f\"Training tokenizer with vocab_size={vocab_size}...\")\n",
    "    os.remove(\"lyric_tokenizer.model\") if os.path.exists(\"lyric_tokenizer.model\") else None\n",
    "    os.remove(\"lyric_tokenizer.vocab\") if os.path.exists(\"lyric_tokenizer.vocab\") else None\n",
    "    user_defined_symbols = [\"<RAP>\", \"<POP>\", \"<ROCK>\", \"<RB>\", \"<COUNTRY>\", \"<V>\"]\n",
    "    \n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        input=mainPath+'train.txt',\n",
    "        model_prefix='lyric_tokenizer',\n",
    "        vocab_size=vocab_size,\n",
    "        model_type='bpe',\n",
    "        character_coverage=1.0,\n",
    "        user_defined_symbols=user_defined_symbols,\n",
    "        pad_id=0,\n",
    "        unk_id=1,\n",
    "        bos_id=2,\n",
    "        eos_id=3,\n",
    "        pad_piece='<pad>',\n",
    "        unk_piece='<unk>',\n",
    "        bos_piece='<s>',\n",
    "        eos_piece='</s>',\n",
    "        normalization_rule_name='nmt_nfkc',\n",
    "        remove_extra_whitespaces=True,\n",
    "        split_by_whitespace=True,\n",
    "        split_by_number=False,\n",
    "        byte_fallback=False,\n",
    "        treat_whitespace_as_suffix=False,\n",
    "        allow_whitespace_only_pieces=False,\n",
    "        max_sentence_length=4192,\n",
    "        num_threads=16\n",
    "    )\n",
    "    \n",
    "    print(\"Tokenizer training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e6da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_files(output_dir=\"./\"):\n",
    "    \"\"\"\n",
    "    Tokenize train and val files\n",
    "    \"\"\"\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(\"lyric_tokenizer.model\")\n",
    "    \n",
    "    def tokenize_file(input_file, output_file):\n",
    "        token_ids_list = []\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if len(line) == 0:\n",
    "                    continue\n",
    "                token_ids = sp.encode(line, out_type=int)\n",
    "                token_ids_list.append(token_ids)\n",
    "        \n",
    "        np.save(output_file, np.array(token_ids_list, dtype=object))\n",
    "        print(f\"Saved {output_file}, total sequences: {len(token_ids_list)}\")\n",
    "    \n",
    "    tokenize_file(mainPath+\"train.txt\", os.path.join(output_dir, \"train_tokens.npy\"))\n",
    "    tokenize_file(mainPath+\"val.txt\", os.path.join(output_dir, \"val_tokens.npy\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e60fcfc",
   "metadata": {},
   "source": [
    "## Lyrics Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc49652",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Create pairs of input-output sequences with genre labels for lyrics generation\n",
    "    \"\"\"\n",
    "    def __init__(self, token_list, sp, seq_len, stride=None):\n",
    "        self.samples = []\n",
    "        self.seq_len = seq_len\n",
    "        self.stride = stride or seq_len\n",
    "        \n",
    "        self.sp_genre_to_idx = {\n",
    "            sp.piece_to_id('<RAP>'): 0,\n",
    "            sp.piece_to_id('<POP>'): 1,\n",
    "            sp.piece_to_id('<ROCK>'): 2,\n",
    "            sp.piece_to_id('<RB>'): 3,\n",
    "            sp.piece_to_id('<COUNTRY>'): 4\n",
    "        }\n",
    "\n",
    "        \n",
    "        for song in token_list:\n",
    "            if len(song) < 3: \n",
    "                continue\n",
    "            \n",
    "            # Second token is genre ID (after <s>)\n",
    "            genre_sp_id = song[1]\n",
    "            genre_idx = self.sp_genre_to_idx.get(genre_sp_id)\n",
    "            \n",
    "            if genre_idx is None:\n",
    "                continue\n",
    "            \n",
    "            # Remove genre tokens from sequence (keep only lyrics)\n",
    "            song_content = song[2:] \n",
    "            L = len(song_content)\n",
    "            \n",
    "            for i in range(0, L - seq_len, self.stride):\n",
    "                chunk = song_content[i:i + seq_len + 1]\n",
    "                \n",
    "                if len(chunk) == seq_len + 1:\n",
    "                    x = chunk[:-1]\n",
    "                    y = chunk[1:]\n",
    "                    self.samples.append((x, y, genre_idx))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, y, g = self.samples[idx]\n",
    "        return (\n",
    "            torch.tensor(x, dtype=torch.long),\n",
    "            torch.tensor(y, dtype=torch.long),\n",
    "            torch.tensor(g, dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedef081",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2c38b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_LyricGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based lyrics generation model with genre conditioning\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, genre_size,\n",
    "                 embed_dim=384, genre_embed_dim=64,\n",
    "                 hidden_size=768, num_layers=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.genre_embedding = nn.Embedding(genre_size, genre_embed_dim)\n",
    "        \n",
    "        self.embed_norm = nn.LayerNorm(embed_dim + genre_embed_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim + genre_embed_dim,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.output_norm = nn.LayerNorm(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize model weights\n",
    "        \"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name and 'norm' not in name:\n",
    "                if len(param.shape) >= 2:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "    \n",
    "    def forward(self, x, genre_id, hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        word_embed = self.embedding(x)\n",
    "        genre_embed = self.genre_embedding(genre_id).unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        \n",
    "        lstm_input = torch.cat([word_embed, genre_embed], dim=2)\n",
    "        lstm_input = self.embed_norm(lstm_input)\n",
    "        \n",
    "        lstm_output, hidden = self.lstm(lstm_input, hidden)\n",
    "        \n",
    "        lstm_output = self.output_norm(lstm_output)\n",
    "        logits = self.fc(lstm_output)\n",
    "        \n",
    "        return logits, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dc2401",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa20dac3",
   "metadata": {},
   "source": [
    "Train one epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, x, y, genre, criterion, accumulation_steps):\n",
    "    \"\"\"Compute loss with mixed precision.\"\"\"\n",
    "    with autocast():\n",
    "        logits, _ = model(x, genre)\n",
    "        loss = criterion(\n",
    "            logits.reshape(-1, logits.size(-1)),\n",
    "            y.reshape(-1)\n",
    "        )\n",
    "        return loss / accumulation_steps\n",
    "\n",
    "\n",
    "def update_model_weights(optimizer, scaler, model):\n",
    "    \"\"\"Update model parameters with gradient clipping.\"\"\"\n",
    "    scaler.unscale_(optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, \n",
    "                    accumulation_steps=4):\n",
    "    \"\"\"Train model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_batches = len(train_loader)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i, (x, y, genre) in enumerate(train_loader):\n",
    "        x, y, genre = x.to(device), y.to(device), genre.to(device)\n",
    "        \n",
    "        loss = compute_loss(model, x, y, genre, criterion, accumulation_steps)\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            update_model_weights(optimizer, scaler, model)\n",
    "        \n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        \n",
    "        if (i + 1) % 500 == 0:\n",
    "            avg_loss = total_loss / (i + 1)\n",
    "            percent = (i + 1) / total_batches * 100\n",
    "            print(f\"  Batch {i+1}/{total_batches} ({percent:.1f}%) - Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af56ca06",
   "metadata": {},
   "source": [
    "Evaluate one epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5f6d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_one_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validation loop with perplexity calculation\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y, genre in val_loader:\n",
    "            x, y, genre = x.to(device), y.to(device), genre.to(device)\n",
    "            \n",
    "            logits, _ = model(x, genre)\n",
    "            loss = criterion(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                y.reshape(-1)\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    perplexity = math.exp(min(avg_loss, 10)) \n",
    "    \n",
    "    return avg_loss, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eca602",
   "metadata": {},
   "source": [
    "Train Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2571c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_model(epoch, model, optimizer, val_loss, val_perplexity, save_path):\n",
    "    \"\"\"Save the best model checkpoint.\"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_loss': val_loss,\n",
    "        'val_perplexity': val_perplexity\n",
    "    }, save_path)\n",
    "    print(\"Saved best model\")\n",
    "\n",
    "\n",
    "def save_periodic_checkpoint(epoch, model, optimizer, save_path):\n",
    "    \"\"\"Save periodic training checkpoint.\"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, save_path)\n",
    "\n",
    "\n",
    "def should_stop_early(patience_counter, patience):\n",
    "    \"\"\"Check if early stopping criteria is met.\"\"\"\n",
    "    return patience_counter >= patience\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, epochs=80, \n",
    "                lr=0.001, weight_decay=1e-5):\n",
    "    \"\"\"Training loop with early stopping and checkpointing.\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Starting Training\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, scaler, device)\n",
    "        val_loss, val_perplexity = eval_one_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"\\n  Train Loss:      {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss:        {val_loss:.4f}\")\n",
    "        print(f\"  Val Perplexity:  {val_perplexity:.2f}\")\n",
    "        print(f\"  Learning Rate:   {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            save_best_model(epoch, model, optimizer, val_loss, val_perplexity, \n",
    "                          mainPath + \"best_model.pt\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if should_stop_early(patience_counter, patience):\n",
    "                print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            save_periodic_checkpoint(epoch, model, optimizer, \n",
    "                                    f\"{mainPath}checkpoint_epoch_{epoch+1}.pt\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Training Complete!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b45104c",
   "metadata": {},
   "source": [
    "## Generation of Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7355e2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_v_tokens_to_newlines(text):\n",
    "    return re.sub(r'\\s*<v>\\s*', '\\n', text)\n",
    "\n",
    "\n",
    "def remove_empty_lines(text):\n",
    "    lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "def clean_generated_text(text):\n",
    "    \"\"\"Post-process generated lyrics text.\"\"\"\n",
    "    text = remove_standalone_punctuation(text)\n",
    "    text = normalize_whitespace(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_duplicate_punctuation(text)\n",
    "    text = remove_quotation_marks(text)\n",
    "    text = convert_v_tokens_to_newlines(text)\n",
    "    text = remove_empty_lines(text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aee824",
   "metadata": {},
   "source": [
    "To generate lyrics, we can pass in the first few words of a song as well as the genre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de2440f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_genre_id(genre_name):\n",
    "    genre_map = {\"rap\": 0, \"pop\": 1, \"rock\": 2, \"rb\": 3, \"country\": 4}\n",
    "    return genre_map[genre_name.lower()]\n",
    "\n",
    "\n",
    "def prepare_initial_tokens(genre_name, prompt, sp, device):\n",
    "    initial_text = f\"<{genre_name.upper()}>\"\n",
    "    if prompt:\n",
    "        initial_text += f\" {prompt}\"\n",
    "    \n",
    "    tokens = sp.encode(initial_text, out_type=int)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def apply_repetition_penalty(logits, generated_tokens, recent_tokens, penalty):\n",
    "    for token in set(generated_tokens[-50:]):\n",
    "        logits[token] /= penalty\n",
    "    \n",
    "    if len(recent_tokens) >= 20:\n",
    "        for token in recent_tokens[-10:]:\n",
    "            if recent_tokens[-20:].count(token) > 3:\n",
    "                logits[token] /= 2.0\n",
    "    \n",
    "    return logits\n",
    "\n",
    "\n",
    "def apply_top_k_filtering(logits, top_k):\n",
    "    if top_k > 0:\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = float('-inf')\n",
    "    return logits\n",
    "\n",
    "\n",
    "def apply_top_p_filtering(logits, top_p):\n",
    "    if top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        \n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        \n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = float('-inf')\n",
    "    \n",
    "    return logits\n",
    "\n",
    "\n",
    "def sample_next_token(logits):\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    return torch.multinomial(probs, 1).item()\n",
    "\n",
    "\n",
    "def should_stop_generation(token, sp):\n",
    "    return token == sp.eos_id() or token == 0\n",
    "\n",
    "\n",
    "def generate_lyrics(model, sp, prompt=\"\", genre_name=\"rap\", max_len=250, \n",
    "                   temperature=0.7, top_k=40, top_p=0.85, repetition_penalty=1.2,\n",
    "                   device='cuda'):\n",
    "    \"\"\"Generate lyrics with top-k/top-p sampling and repetition penalty.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    genre_id = get_genre_id(genre_name)\n",
    "    tokens = prepare_initial_tokens(genre_name, prompt, sp, device)\n",
    "    \n",
    "    genre_tensor = torch.tensor([genre_id], dtype=torch.long).to(device)\n",
    "    generated = tokens.tolist()[0]\n",
    "    recent_tokens = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden = None\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            context = tokens[:, -128:]\n",
    "            \n",
    "            logits, hidden = model(context, genre_tensor, hidden)\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "            \n",
    "            next_token_logits = apply_repetition_penalty(\n",
    "                next_token_logits, generated, recent_tokens, repetition_penalty\n",
    "            )\n",
    "            next_token_logits = apply_top_k_filtering(next_token_logits, top_k)\n",
    "            next_token_logits = apply_top_p_filtering(next_token_logits, top_p)\n",
    "            \n",
    "            next_token = sample_next_token(next_token_logits)\n",
    "            \n",
    "            if should_stop_generation(next_token, sp):\n",
    "                break\n",
    "            \n",
    "            generated.append(next_token)\n",
    "            recent_tokens.append(next_token)\n",
    "            tokens = torch.cat([tokens, torch.tensor([[next_token]]).to(device)], dim=1)\n",
    "    \n",
    "    raw_text = sp.decode(generated)\n",
    "    cleaned_text = clean_generated_text(raw_text)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7ccde2",
   "metadata": {},
   "source": [
    "## Main Execution Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581b4cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Complete pipeline from data preprocessing to model training to generating lyrics samples.\n",
    "    \"\"\"\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    print(\"Preprocessing Data\")\n",
    "    clean_and_filter_lyrics_dataset(\n",
    "        input_csv=mainPath+'song_lyrics.csv',\n",
    "        output_csv=mainPath+'english_cleaned_songs.csv'\n",
    "    )\n",
    "    balance_genres(\n",
    "        input_csv=mainPath+\"english_cleaned_songs.csv\",\n",
    "        output_csv=mainPath+\"english_cleaned_reduced.csv\",\n",
    "        max_per_genre=85000\n",
    "    )\n",
    "\n",
    "    create_train_val_splits(mainPath+\"english_cleaned_reduced.csv\")\n",
    "    \n",
    "    print(\"Training Tokenizer\")\n",
    "    train_tokenizer(vocab_size=16000)\n",
    "    tokenize_files(output_dir=mainPath)\n",
    "\n",
    "    print(\"Loading Data\")\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(\"lyric_tokenizer.model\")\n",
    "    \n",
    "    train_tokens = np.load(mainPath+\"train_tokens.npy\", allow_pickle=True)\n",
    "    val_tokens = np.load(mainPath+\"val_tokens.npy\", allow_pickle=True)\n",
    "    \n",
    "    print(f\"Loaded {len(train_tokens)} training songs\")\n",
    "    print(f\"Loaded {len(val_tokens)} validation songs\")\n",
    "    print(f\"Vocabulary size: {sp.get_piece_size()}\")\n",
    "    \n",
    "    print(\"Creating Datasets\")\n",
    "    seq_len = 128\n",
    "    \n",
    "    train_dataset = LyricsDataset(train_tokens, sp, seq_len=seq_len, stride=64)\n",
    "    val_dataset = LyricsDataset(val_tokens, sp, seq_len=seq_len, stride=seq_len)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=4,\n",
    "        pin_memory=True if DEVICE == \"cuda\" else False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=128,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=4,\n",
    "        pin_memory=True if DEVICE == \"cuda\" else False\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    print(\"Initializing Model\")\n",
    "    model = LSTM_LyricGenerator(\n",
    "        vocab_size=sp.get_piece_size(),\n",
    "        genre_size=5,\n",
    "        embed_dim=384,\n",
    "        genre_embed_dim=64,\n",
    "        hidden_size=768,\n",
    "        num_layers=3,\n",
    "        dropout=0.2\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    print(\"Training Model\")\n",
    "    train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=DEVICE,\n",
    "        epochs=70,\n",
    "        lr=0.001,\n",
    "        weight_decay=1e-5\n",
    "    )\n",
    "     \n",
    "    print(\"Generating Sample Lyrics\")\n",
    "    checkpoint = torch.load(mainPath+\"best_model.pt\", map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "    print(f\"Best validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "    print(f\"Best validation perplexity: {checkpoint['val_perplexity']:.2f}\")\n",
    "    \n",
    "    genres = [\"rap\", \"pop\", \"rock\", \"rb\", \"country\"]\n",
    "    \n",
    "    for genre in genres:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Generated {genre.upper()} Lyrics:\")\n",
    "        print('='*70)\n",
    "        \n",
    "        lyrics = generate_lyrics(\n",
    "            model=model,\n",
    "            sp=sp,\n",
    "            prompt=\"\",\n",
    "            genre_name=genre,\n",
    "            max_len=200,\n",
    "            temperature=0.7,\n",
    "            top_k=40,\n",
    "            top_p=0.85,\n",
    "            repetition_penalty=1.2,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        \n",
    "        print(lyrics)\n",
    "        print()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf399d14",
   "metadata": {},
   "source": [
    "## Usage Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79c5ea7",
   "metadata": {},
   "source": [
    "Generate lyrics after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ec7b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"lyric_tokenizer.model\")\n",
    "\n",
    "model = LSTM_LyricGenerator(vocab_size=sp.get_piece_size(), genre_size=5)\n",
    "checkpoint = torch.load(\"best_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "lyrics = generate_lyrics(\n",
    "    model=model,\n",
    "    sp=sp,\n",
    "    prompt=\"I am the\",\n",
    "    genre_name=\"rap\",\n",
    "    max_len=300\n",
    ")\n",
    "print(lyrics)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
